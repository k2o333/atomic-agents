
基于您这套高度模块化但又有依赖关系的架构，最佳的开发顺序应该遵循**“由内而外、先骨架后血肉、分层构建”**的原则。

我们将整个开发过程划分为**四个主要阶段（Milestones）**，每个阶段都有明确的交付物和验证目标。

---

### **开发顺序路线图**

#### **第一阶段 (M1): 核心骨架与“Hello World”工作流 (预计2-3周)**

**目标**: 搭建起整个系统的最基本骨架，并跑通一个最简单的、单任务的“Hello World”流程。这个阶段的重点是**打通数据流**，而不是实现复杂的AI逻辑。

1.  **【基础设施】核心接口 (`interfaces.py`)**:
    *   **任务**: 定义最基础的Pydantic模型，如`Task`, `Edge`, `AgentResult` (可以先只包含`FinalAnswer`)。
    *   **为什么先做**: 这是所有模块的“通用语言”，必须最先定义。

2.  **【基础设施】数据库服务 (`PersistenceService`)**:
    *   **任务**: 编写连接PostgreSQL的代码，并实现对`tasks`和`edges`表最基本的CRUD操作。
    *   **数据库**: 同时，DBA需要创建好`tasks`和`edges`这两张最核心的表。
    *   **为什么这么早**: 数据库是系统的“心脏”，所有状态都依赖它。

3.  **【基础设施】日志与追踪服务 (`Logging & Tracing Service`)**:
    *   **任务**: 实现一个基础版的、输出结构化JSON日志的服务。可以先不实现复杂的`span_id`，但**必须**有`trace_id`。
    *   **为什么这么早**: 从第一行代码开始就应该有日志，否则后续调试是灾难。

4.  **【核心】中央图引擎 (`Central Graph Engine`) - 简化版**:
    *   **任务**: 编写一个**最简化的、基于轮询（Polling）**的图引擎。它暂时**不**需要事件驱动，也**不**需要处理复杂的`directives`。
    *   **核心逻辑**:
        *   `while True:`循环
        *   查询DB中`status='PENDING'`的任务。
        *   如果任务`assignee`是`Agent`，就调用`AgentService`。
        *   处理`Agent`返回的`FinalAnswer`，更新任务状态为`COMPLETED`。
    *   **为什么用轮询**: 这能让我们在没有事件总线的情况下，最快地验证核心流程。

5.  **【核心】Agent服务 (`AgentService`) - 简化版**:
    *   **任务**: 实现一个能根据`implementation_path`动态加载并运行Agent脚本的执行器。
    *   **依赖**: 需要一个最简单的`GenericWorkerAgent.py`脚本模板。

6.  **【能力】一个“Hello World” Agent**:
    *   **任务**: 编写一个极其简单的`HelloWorldAgent.py`，它的`.run()`方法不调用LLM，只是返回一个硬编码的`{"status": "SUCCESS", "output": {"thought": "...", "intent": {"FinalAnswer": {"content": "Hello World!"}}}}`。
    *   **配置**: 编写一个只包含这个Agent的`capabilities.json`。

**M1验收标准**:
*   能够通过API（或一个简单的脚本）在数据库中手动`INSERT`一个`Task`。
*   `图引擎`能够轮询到这个任务，调用`HelloWorldAgent`，并将任务状态更新为`COMPLETED`，`result`字段中包含了"Hello World!"。
*   整个过程的所有关键步骤都有带`trace_id`的结构化日志输出。

---

#### **第二阶段 (M2): 引入AI智能与工具 (预计3-4周)**

**目标**: 将LLM的“大脑”和Tool的“手臂”接入系统，实现一个真正有用的、包含“思考-行动”循环的任务。

好的，我们来将第二阶段（Milestone 2）的所有开发任务，进行一次**详尽的、可执行的、包含具体开发需求**的分解。

M2的核心目标是**“为系统注入智能”**，让它从一个只能执行硬编码逻辑的“骨架”，进化成一个能够真正利用LLM进行思考、并与外部世界（通过Tool）进行交互的“有机体”。

---

### **M2 开发任务详细分解**

#### **1. 【核心服务】LLM网关服务 (`LLMService`) - 核心版**

*   **开发者**: 负责基础设施的工程师。
*   **产品需求**: FR-LLM-01 (部分), FR-LLM-04 (部分)。
*   **开发任务**:
    1.  **创建`LLMGateway`模块**: 建立`llmgateway/`目录结构。
    2.  **实现配置加载**: 能够从环境变量或配置文件中安全地加载LLM提供商的API Key和基础URL。
    3.  **实现统一接口**: 创建一个`LLMGatewayService`类，提供一个核心方法 `chat_completion(request: OpenAICompatibleRequest) -> OpenAICompatibleResponse`。
    4.  **实现OpenAI客户端**: 在`chat_completion`方法内部，实现对**OpenAI API**的直接调用。
        *   需要处理`model`, `messages`, `stream`等基本参数的传递。
    5.  **实现Tool Calling适配层**:
        *   **对于支持原生Tool Calling的模型**: 能够将`interfaces.py`中定义的工具列表，转换为OpenAI API要求的`tools`参数格式。
        *   **对于不支持的模型 (模拟)**: 能够将工具列表渲染成文本，并注入到提示词中。
    6.  **实现Token计数**: **(必需)**
        *   集成`tiktoken`库。
        *   在调用API**之前**，计算`prompt_tokens`。
        *   在收到API响应**之后**，计算`completion_tokens`。
        *   将`usage`信息（`prompt_tokens`, `completion_tokens`, `total_tokens`）作为响应的一部分返回。
    7.  **实现自动重试**: **(必需)**
        *   使用`tenacity`库，为API调用函数添加`@retry`装饰器。
        *   配置合理的重试策略，例如：指数退避（`wait_exponential`），最多重试3次（`stop_after_attempt(3)`），只在特定的网络或服务器错误（如5xx错误）上重试。
    8.  **编写单元测试**: 模拟OpenAI API的响应和错误，测试Token计数、重试逻辑和Tool Calling适配是否正常工作。

*   **验收标准**:
    *   能够成功调用OpenAI的`chat/completions`端点。
    *   能够正确计算并返回Token使用量。
    *   在模拟的网络错误下，能够看到自动重试的日志。
    *   能够正确地将工具定义传递给支持Tool Calling的模型。

---

#### **2. 【SDK】完善`BaseAgent`**

*   **开发者**: 负责核心框架/SDK的工程师。
*   **产品需求**: 支持Agent与图引擎的标准交互协议。
*   **开发任务**:
    1.  **修改`base_agent.py`**:
    2.  **实现`request_llm_call`**:
        *   创建一个辅助方法 `request_llm_call(self, prompt: str, tools: List = None) -> Dict`。
        *   此方法不执行任何操作，只是构建并返回一个标准的、调用`System.LLM.invoke`的`ToolCallRequest`意图的`AgentResult`字典。
    3.  **实现`request_tool_call`**:
        *   创建一个辅助方法 `request_tool_call(self, tool_id: str, arguments: Dict) -> Dict`。
        *   此方法构建并返回一个调用指定`tool_id`的`ToolCallRequest`意图。
    4.  **实现重入逻辑 (Re-entry Handling)**:
        *   在`BaseAgent`的`.run()`方法或其`__init__`中，增加逻辑来检查`self.task_data['context']`。
        *   提供几个辅助的getter方法，供子类使用：
            *   `get_last_llm_response(self) -> Optional[Dict]`
            *   `get_last_tool_result(self) -> Optional[Dict]`
            *   `is_first_run(self) -> bool`: 判断当前是否是首次执行（即没有`last_...`结果）。
    5.  **提供`FinalAnswer`构建器**:
        *   `create_final_answer(self, thought: str, content: Any) -> Dict`：构建一个包含`FinalAnswer`意图的`AgentResult`。

*   **验收标准**:
    *   自定义Agent可以继承`BaseAgent`并轻松地使用这些辅助方法来返回标准化的意图。
    *   `is_first_run`, `get_last_llm_response`等方法能够正确地从`task_data`中解析出状态。

---

#### **3. 【核心】Agent服务 (`AgentService`) 增强**

*   **开发者**: 负责Agent执行环境的工程师。
*   **产品需求**: FR-AGT-02 (部分)。
*   **开发任务**:
    1.  **实现对`GenericWorkerAgent`的支持**:
        *   在`AgentService`的执行逻辑中，当加载的Agent是`GenericWorkerAgent`时：
        *   **必须**能够从`Group`或`Agent`的JSON配置中，读取`prompt_template`。
        *   **必须**能够使用`Task`的`input_data`来渲染这个模板中的`{variable}`占位符。
        *   `GenericWorkerAgent`的`.run()`方法，其逻辑就是固定的：构建提示词，然后返回`request_llm_call`的意图。

*   **验收标准**:
    *   在`capabilities.json`中只配置一个`GenericWorkerAgent`（无自定义PY脚本），`图引擎`能够成功地执行它，并看到`AgentService`正确地构建了提示词并发起了LLM调用请求。

---

#### **4. 【执行器】原生工具服务 (`ToolService`) - 核心版**

*   **开发者**: 负责Tool执行环境的工程师。
*   **产品需求**: 让系统具备执行外部动作的能力。
*   **开发任务**:
    1.  **创建`ToolService`模块**: 建立`ToolService/`目录结构。
    2.  **实现动态加载器**: 编写一个`ToolExecutor`类，它能根据`tool_id`，从能力注册表中查找`implementation_path`，并使用`importlib`动态加载Tool的`run`函数。
    3.  **实现核心执行方法**: `run_tool(tool_id: str, arguments: Dict) -> Dict`。
        *   此方法负责加载Tool函数。
        *   **必须**对传入的`arguments`与`capabilities.json`中定义的`parameters_schema`进行**校验**（可以使用`jsonschema`库）。
        *   调用Tool函数并传入参数。
        *   捕获任何异常，并将其包装成一个标准的失败结果返回。
    4.  **【注意】初期不实现安全沙箱**: 所有Tool的`run`函数将直接在`ToolService`的进程中执行。**必须**在文档中明确注明这是开发阶段的简化，存在安全风险。

*   **验收标准**:
    *   `图引擎`可以成功调用`ToolService.run_tool()`。
    *   如果传入的参数不符合Schema，`run_tool`会返回一个明确的校验错误。
    *   如果Tool脚本执行成功/失败，`run_tool`会返回正确的成功/失败结果。

---

#### **5. 【核心】中央图引擎 (`Engine`) 增强**

*   **开发者**: 负责核心调度逻辑的工程师。
*   **产品需求**: 让引擎能够理解和处理“思考-行动”循环。
*   **开发任务**:
    1.  **修改`handle_agent_result`方法**:
    2.  **增加对`ToolCallRequest`的处理**:
        *   当`AgentResult.intent`是`ToolCallRequest`时：
            a.  **不**将任务标记为`COMPLETED`。
            b.  调用`ToolService.run_tool()`来执行工具。
            c.  获取`Tool`的返回结果`tool_result`。
            d.  调用`PersistenceService`，`UPDATE`当前`Task`的**上下文（`context`）字段**，将`tool_result`存入其中（例如，`{"last_tool_result": tool_result}`）。
            e.  **重要**: 数据库的`UPDATE`操作会自动触发`NOTIFY`，这将导致**同一个`task_id`**被再次放入任务队列，从而启动Agent的“重入”执行。

*   **验收标准**:
    *   当一个Agent返回`ToolCallRequest`时，可以看到`图引擎`的日志显示它调用了`ToolService`。
    *   可以看到`ToolService`执行完毕后，`tasks`表中的对应任务记录的`context`字段被更新了。
    *   可以看到同一个`task_id`被再次处理，并且这次Agent在执行时，可以从`task_data`中获取到`last_tool_result`。

---

#### **6. 【能力】开发一个真实的Agent和Tool**

*   **开发者**: 应用/业务逻辑开发者。
*   **产品需求**: 提供一个端到端的、有实际意义的演示。
*   **开发任务**:
    1.  **编写`search_weather.py`**:
        *   一个简单的Tool脚本，可以调用一个公开的天气API。
    2.  **编写`WeatherAgent.py`**:
        *   一个继承自`BaseAgent`的自定义Agent。
        *   它的`_generate_dynamic_prompt`会构建一个查询天气的提示。
        *   它的`_handle_llm_response`会解析LLM的输出，判断是否需要调用工具。
        *   它的`run`方法会处理重入逻辑：首次运行时请求LLM，收到LLM的`ToolCallRequest`意图后，自己再返回`ToolCallRequest`；收到Tool的结果后，再请求LLM生成最终的自然语言答案。
    3.  **编写`capabilities.json`**:
        *   注册`search_weather` Tool和`WeatherAgent`。

*   **验收标准**:
    *   M2阶段的**最终端到端测试**：创建一个`goal`为“北京今天天气怎么样？”的`Task`，分配给`WeatherAgent`。
    *   观察整个系统，是否能完整地、正确地执行“Agent思考 -> 请求Tool -> 引擎执行Tool -> 结果反馈给Agent -> Agent再思考 -> 最终回答”的完整循环。
    *   最终`tasks`表中的`result`字段，包含了“北京今天的天气是...”这样的自然语言答案。

**M2验收标准**:
*   能够执行一个分配给`WeatherAgent`的任务。
*   `WeatherAgent`能够成功请求调用`search_weather`工具。
*   `图引擎`能够成功执行Tool，并将结果返回给`WeatherAgent`。
*   `WeatherAgent`能够根据工具结果，生成最终的`FinalAnswer`。
*   `LLMService`能够记录下这次交互的Token用量。

---

#### **第三阶段 (M3): 实现完整的动态工作流 (预计4-5周)**

**目标**: 激活系统的“灵魂”——`Planner`和动态图能力，从执行单个任务进化到编排复杂流程。

1.  **【核心服务】上下文服务 (`ContextService`)**:
    *   **任务**: 实现一个功能完备的上下文服务，能够按需拉取数据、融合提示词、注入约束。
2.  **【核心】中央图引擎 (`Engine`) 终极版**:
    *   **任务**:
        *   实现处理`Planner`返回的`PlanBlueprint`的逻辑（在DB中创建新任务和边）。
        *   实现完整的**`ConditionEvaluator`**（集成CEL库）。
        *   实现处理`Edge`的条件路由逻辑。
3.  **【能力】开发Planner**:
    *   **任务**: 实现`GenericPlannerAgent.py`。编写一个`CentralDecisionGroup`的`Group`配置，并为其设计一个强大的`planner_prompt_template`。
4.  **【基础设施】引入事件驱动 (`Event-Driven Adapters`)**:
    *   **任务**: 编写数据库**触发器**和`Notify Handler`。将`图引擎`从**轮询模式改造为事件驱动模式**。
    *   **为什么现在做**: 因为现在系统流程变复杂了，低延迟变得更重要。

**M3验收标准**:
*   能够向`CentralDecisionGroup`提交一个高层目标（例如，“研究天气并写报告”）。
*   `Planner`能够动态生成一个包含至少两个步骤（研究、写报告）和一个条件边的`PlanBlueprint`。
*   `图引擎`能够正确地创建并按顺序执行这个动态生成的工作流。
*   整个流程由`LISTEN/NOTIFY`驱动，而非轮询。

---

<h4><strong>第四阶段 (M4): 企业级特性与加固 (持续进行)</strong></h4>

**目标**: 为平台增加生产环境所需的可靠性、安全性和可管理性。

1.  **【核心】版本控制与回滚 (`Versioning Engine` & `WorkspaceVersioningService`)**:
    *   **任务**: 实现数据库历史表和触发器，实现基于Git的文件版本化，并为`图引擎`增加回滚和时间旅行的API。
2.  **【核心服务】LLM网关 (`LLMService`) 完整版**:
    *   **任务**: 增加**配额控制**和**请求缓存**功能。
3.  **【执行器】`ToolService` 安全增强**:
    *   **任务**: 将Tool的执行迁移到**安全的Docker沙箱**中。
4.  **【能力】高级模式与边缘情况**:
    *   **任务**: 在`图引擎`中实现对`loop_directive`（并行/循环）和`timeout`指令的支持。
    *   根据“穷举测试清单”逐步实现其他边缘情况的处理。
5.  **【外围】部署与监控**:
    *   **任务**: 编写`Docker Compose`和`Kubernetes`部署文件。集成`Prometheus`和`Grafana`进行监控。

**M4验收标准**:
*   所有核心功能都已实现。
*   系统能够在模拟的生产环境下（例如，通过混沌工程测试）稳定运行。
*   关键的性能和业务指标都可以在监控仪表盘上看到。

这个分阶段的路线图，确保了您的团队在每个阶段都有清晰、可交付、可验证的目标，从一个最简单的“能跑”的骨架，逐步地、有机地“生长”成一个功能完备、坚实可靠的终极平台。